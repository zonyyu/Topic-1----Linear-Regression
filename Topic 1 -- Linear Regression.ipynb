{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3084e773",
   "metadata": {},
   "source": [
    "# Topic 1 -- Linear Regression\n",
    "\n",
    "Welcome everyone! In this workshop, we will learn about one of the fundamental algorithms of supervised learning: Linear Regression. This notebook contains a step-by-step guide on implementing linear regression in SciKitLearn, as well as a collection of pictures and interactive graphs that will give you an intuition on the idea of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32db5b3",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "1. [What is Linear Regression?](#what-is-linear-regression?)\n",
    "    - [The Car Price Problem](#the-car-price-problem)\n",
    "    - [Finding Optimal $w$ and $b$](#finding-optimal)\n",
    "    - [Gradient Descent](#gradient-descent)\n",
    "    \n",
    "    \n",
    "2. [Training a Linear Regression Model](#training)\n",
    "    - [Loading and Visualizing the Dataset](#loading)\n",
    "    - [Fitting the Linear Model](#fitting)\n",
    "    - [Observations](#obs1)\n",
    "    \n",
    "    \n",
    "3. [Linear Regression with Multiple Variables](#multivar)\n",
    "    - [Choosing Features](#choosing)\n",
    "    - [Training our Multi-Variate Linear Regression Model](#training2)\n",
    "    - [Observations](#obs2)\n",
    "    \n",
    "    \n",
    "4. [Polynomial Regression](#poly)\n",
    "    - [Intuition](#intuition)\n",
    "    - [Preparing our Features and Fitting our Model](#features2)\n",
    "    - [Observations](#obs3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bf72e",
   "metadata": {},
   "source": [
    "### Before we begin...\n",
    "\n",
    "There is one module we need to import. This module contains some code to display the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disp_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78989bf1",
   "metadata": {},
   "source": [
    "## What is Linear Regression?<a name=\"what-is-linear-regression?\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc69ac",
   "metadata": {},
   "source": [
    "Linear regression is the simplest form of supervised learning used to tackle **regression problems**. Regression problems are problems where you try to predict a **continuous output** given various input features. Examples include predicting the height of someone given their age, ethnicity, and biological sex. Notice that height can be any value within some reasonable range (reasonable as in there are no humans that are 10 meters tall), or it could be that you predict the age of stones given the amount of carbon-14 present. All these problems can be solved using linear regression.\n",
    "\n",
    "### The Car Price Problem<a name=\"the-car-price-problem\"/>\n",
    "\n",
    "Say you are given a **dataset** containing the **milage** of cars as well as their respective prices. Your task is to **train** a model from this dataset so that in the future, you can input a certain milage, and the model will return the price of that car. The dataset looks something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429af0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_price_vs_mileage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3977b",
   "metadata": {},
   "source": [
    "Disregarding some outliers, you can see that there is a downwards trend to the data. Now the question is ***How do we predict the price of a car given its mileage in km?*** Linear regression will try to predict a function that best **fits** the data. In the future, we will use this word \\\"**fit**\\\" to reference the training of a machine learning model. And that is really what \\\"training a ML algorithm\\\" is doing -- it is trying to fit a function to the training data! For the most basic linear regression problem, we have one input variable $x$, and one output variable $y$. The function that linear regression will be fitting is shown below:\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>&nbsp;</div>\n",
    "    $\\hat{y} = wx + b$\n",
    "</div>\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ is your hypothesis function, AKA the function you are trying to fit\n",
    "- $x$ is the input data\n",
    "- $w$ is the weight (slope)\n",
    "- $b$ is the bias (y-intercept)\n",
    "\n",
    "In linear regression, the weights and bias are the **parameters** we get to tinker with. By changing the weights and bias, you change the nature of the function. See for your self by playing around with the weight and bias sliders below, and try to fit the function to the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ddd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pvm_with_sliders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d93237",
   "metadata": {},
   "source": [
    "By playing around with the sliders, you have fit a function to the data. This is basically what all supervised learning algorithms are doing, just that usually they are fitting much, **MUCH** more complicated functions. Once you fit a function, you can use that function to make predictions! For example, if you input a mileage of 300,000 km, this function will output a price of around \\$5000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c1a4c",
   "metadata": {},
   "source": [
    "### Finding the Optimal $w$ and $b$ <a name=\"finding-optimal\"/>\n",
    "\n",
    "You might have the question: *How does a machine figure out which $w$ and $b$ are the correct values?* We will need a way of measuring how \\\"wrong\\\" a hypothesis function is. The measurement of error is done with a **cost function**. The cost function for linear regression is as shown:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>&nbsp;</div>\n",
    "    $C(w, b) = {1\\over m} \\sum\\limits_{i=1}^m (\\hat{y}^{(i)}-y^{(i)})^2$\n",
    "</div>\n",
    "\n",
    "Ok, this may look a little complicated, so we will just refer to this cost function as $MSE$, which stands for **Mean Squared Error**. This is the preferred way of representing the cost for linear regression. \n",
    "\n",
    "#### Intuition of the cost function\n",
    "\n",
    "This cost function effectively adds up all the distances of each point to the line, squared. This is useful, as the farther away the linear function is from all the points, the greater the distance, and therefore the greater the error. By minimizing the error, you can find the best fitting $w$ and $b$.\n",
    "\n",
    "### Gradient Descent <a name=\"gradient-descent\"/>\n",
    "\n",
    "Gradient descent is a powerful algorithm that **iteratively** tries to minimize the cost function. To understand gradient descent, lets take a look at the cost function for this car price example:\n",
    "\n",
    "<img src=\"images/gradient-descent.png\" alt=\"image cannot be displayed\" style=\"width: 700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8abfd",
   "metadata": {},
   "source": [
    "To minimize the cost function, we want to travel **down** the slope towards the minimum. And that's how gradient descent gets its name! Gradient is a fancy word for \\\"slope\\\", and we descend down the gradient to find the point of least error.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674da900",
   "metadata": {},
   "source": [
    "## Training a Linear Regression Model <a name=\"training\">\n",
    "\n",
    "Now that we went over the basic concepts of linear regression, we will construct a basic linear regression model in SciKitLearn to predict the price of cars given their mileage. The first step is to import the necessary libraries:\n",
    "\n",
    "- **Numpy**: a powerful linear algebra library\n",
    "- **Pandas**: creates dataframes for organization and visualization\n",
    "- **SKLearn**: Machine Learning framework to train our Linear Regression model.\n",
    "- **MatPlotLib** and **Bokeh**: Data visualization libraries\n",
    "\n",
    "Lets load in these modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b9722",
   "metadata": {},
   "source": [
    "### Loading and Visualizing the Dataset <a name=\"loading\">\n",
    "\n",
    "The first step in training a linear regression (or any machine learning) algorithm is to figure out what kind of data we have to work with. Let's read in `cars.csv` and load it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c45d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700ac62",
   "metadata": {},
   "source": [
    "Taking a look at our dataset, we can see many features such as `odometer_value`, `year_produced`, `engine_capacity`, etc. For our first linear regression model, we will only focus on the columns `odometer_value` and `price_usd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d9e48",
   "metadata": {},
   "source": [
    "Now that we isolated the two variables that we want, we can turn these variables into Numpy arrays to train on. These arrays should be split into a **training set** as well as a **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a17b2",
   "metadata": {},
   "source": [
    "### Fitting the Linear Model <a name=\"fitting\">\n",
    "\n",
    "Now the data is ready for use in training. The first linear regression model is going to be a simple, **single variable** model with no fancy bells and whistles on board. Training will be done using SKLearn's `LinearRegression` model which provides an easy, abstracted interface to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558a9e9",
   "metadata": {},
   "source": [
    "The Accuracy is very low, thats terrible! Maybe plotting the predictions by our model as well as the testing set data can give us a clue on what's going on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17caffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_predictions(X_gt, Y_gt, X_pred, Y_pred):\n",
    "    \"\"\"\n",
    "    Displays predictions vs ground truth\n",
    "    Args: \n",
    "        X_gt, Y_gt: ground truth data and labels\n",
    "        X_pred, Y_pred: test input and predicted outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193437c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c199ffe",
   "metadata": {},
   "source": [
    "### Observations <a name=\"obs1\">\n",
    "\n",
    "By plotting our testing data as well as our model prediction, we are able to see the problem: While the linear function fit looks normal, **there is too much variance in our data**. This is the possible reason why our model is reporting such a low accuracy. Let's keep track of our observations in the observation table down below:\n",
    "\n",
    "| Model | Observation | R2 |\n",
    "| :----- | :----------- | :-------- |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c526b66",
   "metadata": {},
   "source": [
    "## Linear Regression with Multiple Variables <a name=\"multivar\">\n",
    "\n",
    "So far, we have seen linear regression with a single variable in action. The results were not too spectacular, and that is because price of the car is a variable of **multiple features**. For example, the price of the car is not only determined by **mileage**, but also by **engine size**, the **year** it was produced, whatever **special features** it might have.\n",
    "\n",
    "For multivariate linear regression, we will extend what we learned in single variable linear regression to produce a more powerful machine learning algorithm. With multiple input variables, multivariate linear regression would aim to fit a hypothesis function as shown:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>&nbsp;</div>\n",
    "    $\\hat{y} = w_1x_1 + w_2x_2 + ... + w_nx_n + b$\n",
    "</div>\n",
    "\n",
    "### Choosing features <a name=\"choosing\">\n",
    "\n",
    "Lets once again take a look at our dataset, as well as all the columns it currently has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169e10b",
   "metadata": {},
   "source": [
    "You'll probably notice that there are a lot of columns where the values are non-numeric. Many of those features are not important and we will not use them, however for the columns we are using we need to make sure that they are numeric. To keep things simple, we will use **4 features**: `odometer_value`, `year_produced`, `engine_capacity`, and `drivetrain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e455d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe6aa9",
   "metadata": {},
   "source": [
    "### Training our Multi-Variate Linear Regression Model <a name=\"training2\">\n",
    "\n",
    "With our features chosen, now we can train our linear regression model. The training process is quite similar to before; we will create a train-test split and then train using `LinearRegression`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8515eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc70d0",
   "metadata": {},
   "source": [
    "### Observations <a name=\"obs2\">\n",
    "\n",
    "This is looking much better! Because our price is dependent on **multiple features**, it only makes sense to perform multi-variate linear regression on it. Lets fill out our observations table with our new findings.\n",
    "\n",
    "| Model | Observation | R2 |\n",
    "| :----- | :----------- | :-------- |\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b061ab",
   "metadata": {},
   "source": [
    "## Polynomial Regression <a name=\"poly\">\n",
    "\n",
    "Polynomial Regression take linear regression with multiple features one step further by introducing some features as the previous feature raised to some power. This allows us to fit non-linear functions, and since many real world problems have a non-linear relationship, polynomial regression allows us to train models that better fit those relationships\n",
    "\n",
    "### Intuition <a name=\"intuition\">\n",
    "\n",
    "Say we used the single variable linear regression example from above. We can turn that linear regression problem into a polynomial regression problem by introducing a few more features:\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>&nbsp;</div>\n",
    "    $\\hat{y} = w_1x_1 + w_2x_2 + w_3x_3 + b$\n",
    "</div>\n",
    "\n",
    "However, instead of letting $x_2$ and $x_3$ be independent features, we will define them in terms of $x_1$:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>&nbsp;</div>\n",
    "    $x_2 = x_1^2, \\ \\ \\ \\ x_3 = x_1^3$\n",
    "</div>\n",
    "\n",
    "Therefore, the final form of the hypothesis function is as shown. Note that we renamed $x_1$ as just $x$:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <div>&nbsp;</div>\n",
    "    $\\hat{y} = w_1x + w_2x^2 + w_3x^3 + b$\n",
    "</div>\n",
    "\n",
    "With polynomial regression, we are able to fit more complicated non-linear functions to our data. The image below shows the example of a function that is possible with polynomial regression. Note the non-linearity shown by the curves in the surface:\n",
    "\n",
    "<img src=\"images/Polynomial-Regression.png\" alt=\"Cannot display image\" style=\"width:700px\">\n",
    "\n",
    "\n",
    "\n",
    "See for yourselves by playing around with these sliders. Try to find a combination of $w_1$, $w_2$, $w_3$ that fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_poly()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245bdb8",
   "metadata": {},
   "source": [
    "### Preparing our Features and Fitting our Model<a name=\"features2\">\n",
    "\n",
    "To train a polynomial regression model, we first need to obtain the training data, and then convert each feature into many polynomial features. Luckily, there is a handy function in SKLearn called `PolynomialFeatures` that will transform our linear features into polynomial ones. Here, we will define a polynomial transform object with degree of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6585488",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8322202c",
   "metadata": {},
   "source": [
    "### Observations<a name=\"obs3\">\n",
    "\n",
    "We have achieved our highest $R^2$ value yet! Using polynomial regression, we were able to fit a non-linear function much closer to our data than we were able to do with linear functions. Finally, lets fill out our observation table:\n",
    "\n",
    "| Model | Observation | R2 |\n",
    "| :----- | :----------- | :-------- |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e9da04",
   "metadata": {},
   "source": [
    "## $\\mathcal{Fin}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb071ab6",
   "metadata": {},
   "source": [
    "Congrats on finishing the first notebook on our Beginner AI Course! Join the instructor in our next activity as we put what we learned about linear regression to practice!\n",
    "\n",
    "<img src=\"images/umaru.png\" alt=\"Cannot display Image\" style=\"width:700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca4d877",
   "metadata": {},
   "source": [
    "## References <a name=\"references\">\n",
    "    \n",
    "1. [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "2. [https://www.coursera.org/specializations/deep-learning](https://www.coursera.org/specializations/deep-learning)\n",
    "3. [https://machinelearningmastery.com/linear-regression-for-machine-learning/](https://machinelearningmastery.com/linear-regression-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165739f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
